{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Iyg6btLW9M"
   },
   "source": [
    "#  Assignment 2 - Transfer Learning and Data Augmentation 💬\n",
    "\n",
    "Welcome to the **second assignment** for the **CS-552: Modern NLP course**!\n",
    "\n",
    "> - 😀 Name: **Aziz Laadhar**\n",
    "> - ✉️ Email: **aziz.laadhar@epfl.ch**\n",
    "> - 🪪 SCIPER: **315196**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XjnQhbFIJUu"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid green;background-color:#e4fae4;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Assignment Description**\n",
    "- In the first part of this assignment, you will need to implement training (finetuning) and evaluation of a pre-trained language model ([RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)) on a **Sentiment Analysis (SA)** task, which aims to determine whether a product review's emotional tone is positive or negative.\n",
    "\n",
    "- For part-2, following the first finetuning task, you will need to identify the shortcuts (i.e. some salient or toxic features) that the model learnt for the specific task.\n",
    "\n",
    "- For part-3, you are supposed to annotate 80 randomly assigned new datapoints as ground-truth labels. Additionally, the cross annotation should be conducted by another one or two annotators, and you will learn about how to calculate the agreement statistics as a significant characteristic reflecting the quality of a collected dataset.\n",
    "\n",
    "- For part-4, since the human annotation is quite time- and effort-consuming, there are plenty of ways to get silver-labels from automatic labeling to augment the dataset scale, e.g., paraphrasing each text input in different words without changing its meaning. You will use a [T5](https://huggingface.co/docs/transformers/en/model_doc/t5) paraphrase model to expand the training data of sentiment analysis, and evaluate the improvement of data augmentation.\n",
    "\n",
    "For Parts 1 and Part 2, you will need to complete the code in the corresponding `.py` files (`sa.py` for Part 1, `shortcut.py` for Part 2). You will be provided with the function descriptions and detailed instructions about the code snippet you need to write.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "- **PART 1: Sentiment Analysis (33 pts)**\n",
    "    - 1.1 Dataset Processing (10 pts)\n",
    "    - 1.2 Model Training and Evaluation (18 pts)\n",
    "    - 1.3 Fine-Grained Validation (5 pts)\n",
    "- **PART 2: Identify Model Shortcuts (22 pts)**\n",
    "    - 2.1 N-gram Pattern Extraction (6 pts)\n",
    "    - 2.2 Distill Potentially Useful Patterns (8 pts)\n",
    "    - 2.3 Case Study (8 pts)\n",
    "- **PART 3: Annotate New Data (25 pts)**\n",
    "    - 3.1 Write an Annotation Guideline (5 pts)\n",
    "    - 3.2 Annotate Your Datapoints with Partner(s) (8 pts)\n",
    "    - 3.3 Agreement Measure (12 pts)\n",
    "- **PART 4: Data Augmentation (20 pts)**\n",
    "    - 4.1 Data Augmentation with Paraphrasing (15 pts)\n",
    "    - 4.2 Retrain RoBERTa Model with Data Augmentation (5 pts)\n",
    "    \n",
    "### Deliverables\n",
    "\n",
    "- ✅ This jupyter notebook: `assignment2.ipynb`\n",
    "- ✅ `sa.py` and `shortcut.py` file\n",
    "- ✅ Checkpoints for RoBERTa models finetuned on original and augmented SA training data (Part 1 and Part 4), including:\n",
    "    - `models/lr1e-05-warmup0.3/`\n",
    "    - `models/lr2e-05-warmup0.3/`\n",
    "    - `models/augmented/lr1e-05-warmup0.3/`\n",
    "- ✅ Model prediction results on each domain data (Part 1.3 Fine-Grained Validation): `predictions/`\n",
    "- ✅ Cross-annotated new SA data (Part 3), including:\n",
    "    - `data/<your_assigned_dataset_id>-<your_sciper_number>.jsonl`\n",
    "    - `data/<your_assigned_dataset_id>-<your_partner_sciper_number>.jsonl`\n",
    "    - (for group of 3) `data/<your_assigned_dataset_id>-<your_second_partner_sciper_number>.jsonl`\n",
    "- ✅ Paraphrase-augmented SA training data (Part 4), including:\n",
    "    - `data/augmented_train_sa.jsonl`\n",
    "- ✅ `./tensorboard` directory with logs for all trained/finetuned models, including:\n",
    "    - `tensorboard/part1_lr1e-05/`\n",
    "    - `tensorboard/part1_lr2e-05/`\n",
    "    - `tensorboard/part4_lr1e-05/`\n",
    "\n",
    "### How to implement this assignment\n",
    "\n",
    "Please read carefully the following points. All the information on how to read, implement and submit your assignment is explained in details below:\n",
    "\n",
    "1. For this assignment, you will need to implement and fill in the missing code snippets for both the **Jupyter Notebook `assignment2.ipynb`** and the **`sa.py`**, **`shortcut.py`** python files.\n",
    "\n",
    "2. Along with above files, you need to additionally upload model files under the **`models/`** dir, regarding the following models:\n",
    "    - finetuned RoBERTa models on original SA training data (PART 1)  \n",
    "    - finetuned RoBERTa model on augmented SA training data (PART 4)\n",
    "\n",
    "3. You also need to upload model prediction results in Part 1.3 Fine-Grained Validation, saved in **`predictions/`**.\n",
    "\n",
    "4. You also need to upload new data files under the **`data/`** dir (along with our already provided data), including:\n",
    "    - new SA data with your and your partner's annotations (Part 3)\n",
    "    - paraphrase-augmented SA training data (Part 4)\n",
    "\n",
    "5. Finally, you will need to log your training using Tensorboard. Please follow the instructions in the `README.md` of the **``tensorboard/``** directory.\n",
    "\n",
    "**Note**: Large files such as model checkpoints and logs should be pushed to the repository with Git LFS. You may also find that training the models on a GPU can speed up the process, we recommend using Colab's free GPU service for this. A tutorial on how to use Git LFS and Colab can be found [here](https://github.com/epfl-nlp/cs-552-modern-nlp/blob/main/Exercises/tutorials.md).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iJ_9sA2KyP2"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Environment Setup**\n",
    "\n",
    "### **Option 1: creating your own environment**\n",
    "\n",
    "```\n",
    "conda create --name mnlp-a2 python=3.10\n",
    "conda activate mnlp-a2\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note**: If some package versions in our suggested environment do not work, feel free to try other package versions suitable for your computer, but remember to update ``requirements.txt`` and explain the environment changes in your notebook (no penalty for this if necessary).\n",
    "\n",
    "### **Option 2: using Google Colab**\n",
    "If you are using Google Colab notebook for this assignment, you will need to run a few commands to set up our environment on Google Colab, as shown below:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell makes sure modules are auto-loaded when you change external python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfVHqiSvK1aB"
   },
   "outputs": [],
   "source": [
    "# # If you are working in Colab, then consider mounting your assignment folder to your drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # Direct to your assignment folder.\n",
    "# %cd /content/drive/MyDrive/path-to-your-assignment-folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LcGSuvWtmPf"
   },
   "source": [
    "Install packages that are not included in the Colab base envrionemnt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SFXMx5FXtZhQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: transformers==4.38.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (4.38.1)\n",
      "Requirement already satisfied: nltk==3.8.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (1.2.2)\n",
      "Requirement already satisfied: huggingface-hub==0.20.3 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.20.3)\n",
      "Requirement already satisfied: numpy==1.25.2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (1.25.2)\n",
      "Requirement already satisfied: tqdm==4.66.2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (4.66.2)\n",
      "Requirement already satisfied: scipy==1.11.4 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
      "Requirement already satisfied: urllib3==2.0.7 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (2.0.7)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard==2.15.2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (2.15.2)\n",
      "Requirement already satisfied: jsonlines==4.0.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: filelock in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from torch==2.1.0->-r requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from transformers==4.38.1->-r requirements.txt (line 2)) (0.4.2)\n",
      "Requirement already satisfied: click in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 4)) (3.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.62.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (2.28.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.5.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (69.1.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from tensorboard==2.15.2->-r requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from jsonlines==4.0.0->-r requirements.txt (line 12)) (23.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (1.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard==2.15.2->-r requirements.txt (line 11)) (7.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests->transformers==4.38.1->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard==2.15.2->-r requirements.txt (line 11)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from sympy->torch==2.1.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.15.2->-r requirements.txt (line 11)) (3.17.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.15.2->-r requirements.txt (line 11)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard==2.15.2->-r requirements.txt (line 11)) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # limiting to one GPU\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10d14f1f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jsonlines\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "# TODO: Enter your Sciper number\n",
    "SCIPER = '315196'\n",
    "seed = int(SCIPER)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good to go!\n"
     ]
    }
   ],
   "source": [
    "# Check the availability of GPU (proceed only it returns True!)\n",
    "if torch.backends.mps.is_available():\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHhgkhaH-IUl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "    \n",
    "# PART 1: Sentiment Analysis (33 pts)\n",
    "\n",
    "In this part, we will finetune a pretrained language model (Roberta) on sentiment analysis(SA) task. \n",
    "\n",
    "> Specifically, we will focus on a binary sentiment classification task for multi-domain product reviews. It requires the model to **classify a given paragraph of review by its sentiment polarity (positive or negative)**. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tD2YPuqeIYBN"
   },
   "source": [
    "### Load Training Dataset (`train_sa.jsonl`) \n",
    "\n",
    "**You can run the following cell to have the first glance at your data**. Each data sample is a python dictionary, which consists of following components:\n",
    "- input review (*'review'*): a natural language sentence or a paragraph commenting about a product.\n",
    "- domain (*'domain'*): describing the type of product being reviewed.\n",
    "- label of sentiment (*'label'*): indicating whether the review states positive or negative views about the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p-ODgcNUqYtm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review': \"THis book was horrible.  If it was possible to rate it lower than one star i would have.  I am an avid reader and picked this book up after my mom had gotten it from a friend.  I read half of it, suffering from a headache the entire time, and then got to the part about the relationship the 13 year old boy had with a 33 year old man and i lit this book on fire.  One less copy in the world...don't waste your money. I wish i had the time spent reading this book back so i could use it for better purposes.  THis book wasted my life\", 'domain': 'books', 'label': 'negative'}\n",
      "{'review': 'Sphere by Michael Crichton is an excellant novel. This was certainly the hardest to put down of all of the Crichton novels that I have read. The story revolves around a man named Norman Johnson. Johnson is a phycologist. He travels with 4 other civilans to a remote location in the Pacific Ocean to help the Navy in a top secret misssion. They quickly learn that under the ocean is a half mile long spaceship. The civilans travel to a center 1000 feet under the ocean to live while researching the spacecraft. They are joined by 5 Navy personel to help them run operations. However on the surface a typhoon comes and the support ships on the surface must leave. The team of ten is stuck 1000 feet under the surface of the ocean. After a day under the sea they find out that the spacecraft is actually an American ship that has explored black holes and has brought back some strange things back to earth. This novel does not have the research that some of the other Crichton novels have, but it still has a lot of information on random things from the lawes of partial pressure to behavior analysis. I would strongly recommend this book', 'domain': 'books', 'label': 'positive'}\n",
      "{'review': \"This entire movie could have run in only 20 minutes and you wouldn't miss anything and might even enjoy it. Unfortunately it ran 88 minutes too long and I couldn't wait for it to end.  I saw it in the theater and the people all around me were all complaining how boring it was. At least a quarter of them walked out before the end. It's that bad. It's a shame, I love a good suspense/horror movie and the decent actors in this movies were waisted\", 'domain': 'dvd', 'label': 'negative'}\n",
      "{'review': \"I'm not sure why Sony, which now owns I Dream of Jeannie, decided to colorize the first season of this series.  Whatever the reason, you can readily tell by looking at the prices here on Amazon.com that the original black-and-white version of the first season is worth a lot more.  The reason for that is simple--I Dream of Jeannie was originally broadcast in black-and-white.  And for a television fan like myself, that's the ONLY way to watch the first season. The episodes themselves are just as I remember seeing them.  Since I wasn't around in 1965, I'm pretty sure I've never seen these without the cuts that have been referenced here.  But to me, they're still pretty good.  The theme music, in my opinion, is every bit as good as the second theme, introduced when Jeannie went to color in 1966. The one thing that truly will drive the purists nuts is the fact that Sony stripped off the old Screen Gems animation from the end of every episode.  That logo was attached to so many classic shows from the 1960s and 1970s, and it is consistenly rated, along with Viacom's old blue V of Doom, as the scariest logo in the history of television.  The new Sony outro doesn't pack the same punch. Still, if you liked Jeannie way back when, you'll love it now, especially since you can watch it anytime you like, without commercial interruption\", 'domain': 'dvd', 'label': 'positive'}\n",
      "{'review': 'cons tips extremely easy on carpet and if you have a lot of cds stacked at the top poorly designed, it is a vertical cd rack that doesnt have individual slots for cds, so if you want a cd from the bottom of a stack you have basically pull the whole stack to get to it putting it together was a pain, the one i bought i had to break a piece of metal just to fit it in its guide holes. again..poorly designed... doesnt even fit cds that well, there are gaps, and the cd casses are loose fitting pros .......... i guess it can hold a lot of cds....', 'domain': 'electronics', 'label': 'negative'}\n",
      "{'review': 'I purchased this unit due to frequent blackouts in my area and 2 power supplies going bad.  It will run my cable modem, router, PC, and LCD monitor for 5 minutes.  This is more than enough time to save work and shut down.   Equally important, I know that my electronics are receiving clean power. I feel that this investment is minor compared to the loss of valuable data or the failure of equipment due to a power spike or an irregular power supply. As always, Amazon had it to me in &lt;2 business days', 'domain': 'electronics', 'label': 'positive'}\n",
      "{'review': \"He just looks away from where the spray emits--and barks again! It also doesn't work 100% of the time...and we're not sure why.  When we fill it, it seems to work fairly well right after but it either does not have as many sprays as it is supposed to, or it isn't working very long. It does work well for my other small dog who is not such a persistent barker.  Terriers are just too stubborn to care if they're getting sprayed, I guess.\", 'domain': 'housewares', 'label': 'negative'}\n",
      "{'review': 'For those of you unfamiliar with the \"A Series of Unfortunate Events\" books, they detail the absurdly tragic lives of the fictional Baudelaire orphans, and their struggles to overcome adversity (after adversity, after adversity, ad infinitum).  Hovering in the background of their tragedies and occasional, brief glimpses of happiness, is their distant cousin, the scheming, greedy, conniving, ruthless cousin, the Count. Given the concept of the story, this poster fits perfectly, with the orphans, standing together but otherwise alone, in the middle, with the shadow of the Count casting gloom upon them.  It is a perfect fit, as is the casting of Jim Carrey as the Count', 'domain': 'housewares', 'label': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    for sid, sample in enumerate(reader.iter()):\n",
    "        if sid % 200 == 0:\n",
    "            print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "BvM8jd_3QObg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# We use the following pretrained tokenizer and model\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCETOFT2dB4u"
   },
   "source": [
    "## 🎯 Q1.1: **Dataset Processing (10 pts)**\n",
    "\n",
    "Our first step is to constructing a Pytorch Dataset for SA task. Specifically, we will need to implement **tokenization** and **padding** using a HuggingFace pre-trained tokenizer.\n",
    "\n",
    "**TODO🔻: Complete `SADataset` class following the instructions in `sa.py`, and test by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_5Sya9W5BTDl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2034.90it/s]\n"
     ]
    }
   ],
   "source": [
    "from sa import SADataset\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "dataset = SADataset(\"data/train_sa.jsonl\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SADataset test correct ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/aziz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from testA2 import test_SADataset\n",
    "test_SADataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0weQpG6_3vO"
   },
   "source": [
    "## 🎯 Q1.2: **Model Training and Evaluation (18 pts)**\n",
    "\n",
    "Next, we will implement the training and evaluation process to finetune the model. \n",
    "\n",
    "- For training: you will need to calculate the **loss** and update the model weights by using **Adam optimizer**. Additionally, we add a **learning rate schedular** to adopt an adaptive learning rate during the whole training process.\n",
    "\n",
    "- For evaluation: you will need to compute the **confusion matrix** and **F1 scores** to assess the model performance.\n",
    "\n",
    "**TODO🔻: Complete the `compute_metrics()`, `train()` and `evaluate()` functions following the instructions in the `sa.py` file, you can test compute_metrics() by running the following cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6w7Leraw4tIY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 1.]\n",
      " [3. 1.]]\n",
      "0.6\n",
      "compute_metric test correct ✅\n"
     ]
    }
   ],
   "source": [
    "from sa import compute_metrics, train, evaluate\n",
    "\n",
    "from testA2 import test_compute_metrics\n",
    "test_compute_metrics(compute_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvCUS748_3vS"
   },
   "source": [
    "#### **Start Training and Validation!**\n",
    "\n",
    "TODO🔻: (1) [coding question] Train the model with the following two different learning rates (other hyperparameters should be kept consistent). \n",
    "\n",
    "> A. learning_rate = 1e-5\n",
    "\n",
    "> B. learning_rate = 2e-5\n",
    "\n",
    "**Note:** *Each training will take ~7-10 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zn66mMOj_3vS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 4\n",
    "max_grad_norm = 1.0\n",
    "warmup_percent = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Z5aWqR1h_3vS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "207it [00:00, 2059.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 2918.48it/s]\n",
      "Training: 100%|██████████| 200/200 [01:10<00:00,  2.83it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.054 | Validation Loss: 0.889\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2806.  394.]\n",
      " [ 401. 2799.]]\n",
      "F1: (87.59%, 87.56%) | Macro-F1: 87.58%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.96it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:56<00:00, 14.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.054 | Validation Loss: 0.710\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2811.  389.]\n",
      " [ 245. 2955.]]\n",
      "F1: (89.87%, 90.31%) | Macro-F1: 90.09%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.95it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.024 | Validation Loss: 1.041\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2474.  726.]\n",
      " [ 118. 3082.]]\n",
      "F1: (85.43%, 87.96%) | Macro-F1: 86.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.96it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.038 | Validation Loss: 0.686\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2832.  368.]\n",
      " [ 279. 2921.]]\n",
      "F1: (89.75%, 90.03%) | Macro-F1: 89.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-5  # play around with this hyperparameter\n",
    "\n",
    "dev_dataset = SADataset(\"data/test_sa.jsonl\", tokenizer)\n",
    "train(train_dataset=dataset,dev_dataset=dev_dataset, model=model, device=device, batch_size=batch_size, epochs= epochs, learning_rate=learning_rate, warmup_percent=warmup_percent, max_grad_norm=max_grad_norm,\n",
    "      model_save_root='models/', tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 3045.47it/s]\n",
      "Training: 100%|██████████| 200/200 [01:10<00:00,  2.85it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.163 | Validation Loss: 0.583\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2929.  271.]\n",
      " [ 571. 2629.]]\n",
      "F1: (87.43%, 86.20%) | Macro-F1: 86.81%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.126 | Validation Loss: 0.481\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2780.  420.]\n",
      " [ 393. 2807.]]\n",
      "F1: (87.24%, 87.35%) | Macro-F1: 87.30%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.96it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:56<00:00, 14.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.121 | Validation Loss: 0.976\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2311.  889.]\n",
      " [ 117. 3083.]]\n",
      "F1: (82.13%, 85.97%) | Macro-F1: 84.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [01:07<00:00,  2.97it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.118 | Validation Loss: 0.760\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2716.  484.]\n",
      " [ 244. 2956.]]\n",
      "F1: (88.18%, 89.04%) | Macro-F1: 88.61%\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5  # play around with this hyperparameter\n",
    "\n",
    "dev_dataset = SADataset(\"data/test_sa.jsonl\", tokenizer)\n",
    "train(train_dataset=dataset,dev_dataset=dev_dataset, model=model, device=device, batch_size=batch_size, epochs= epochs, learning_rate=learning_rate, warmup_percent=warmup_percent, max_grad_norm=max_grad_norm,\n",
    "      model_save_root='models/', tensorboard_path=\"./tensorboard/part1_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO🔻: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "- Which learning rate is better? Explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first learning rate (1e-5) is better since it achieved higher Macro-F1 score than the second (2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGuzGJCB_3vT"
   },
   "source": [
    "## 🎯 Q1.3: **Fine-Grained Validation (5 pts)**\n",
    "\n",
    "TODO🔻: (1) [coding question] Use the model checkpoint trained from the first learning_rate setting (lr=1e-5), check the model performance on each domain subsets of the validation set. You should report **the validation loss**, **confusion matrix**, **F1 scores** and **Macro-F1 on each domain**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YCWWJjTP_3vT"
   },
   "outputs": [],
   "source": [
    "# Split the test sets into subsets with different domains\n",
    "# Save the subsets under 'data/'\n",
    "# Replace \"...\" with your code\n",
    "domain_data = {}\n",
    "\n",
    "# split to subsets\n",
    "with jsonlines.open(\"data/test_sa.jsonl\", mode=\"r\") as reader:\n",
    "    for sample in reader:\n",
    "        domain = sample[\"domain\"]\n",
    "        if domain not in domain_data:\n",
    "            domain_data[domain] = []\n",
    "        domain_data[domain].append(sample)\n",
    "\n",
    "for domain, samples in domain_data.items():\n",
    "    with jsonlines.open(\"data/test_sa_\"+domain+\".jsonl\", mode=\"w\") as writer:\n",
    "        for sd in samples:\n",
    "            writer.write(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q4J2pu60xHTd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "125it [00:00, 1243.69it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:01, 1508.97it/s]\n",
      "Evaluation:   0%|          | 0/200 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Evaluation: 100%|██████████| 200/200 [00:14<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: books\n",
      "Validation Loss: 0.520\n",
      "Confusion Matrix:\n",
      "[[739.  61.]\n",
      " [102. 698.]]\n",
      "F1: (90.07%, 89.54%) | Macro-F1: 89.81%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 2075.67it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:14<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: dvd\n",
      "Validation Loss: 0.633\n",
      "Confusion Matrix:\n",
      "[[715.  85.]\n",
      " [112. 688.]]\n",
      "F1: (87.89%, 87.48%) | Macro-F1: 87.68%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 3334.37it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:14<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: electronics\n",
      "Validation Loss: 0.548\n",
      "Confusion Matrix:\n",
      "[[723.  77.]\n",
      " [ 92. 708.]]\n",
      "F1: (89.54%, 89.34%) | Macro-F1: 89.44%\n",
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1600it [00:00, 3926.11it/s]\n",
      "Evaluation: 100%|██████████| 200/200 [00:14<00:00, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: housewares\n",
      "Validation Loss: 0.438\n",
      "Confusion Matrix:\n",
      "[[746.  54.]\n",
      " [ 77. 723.]]\n",
      "F1: (91.93%, 91.69%) | Macro-F1: 91.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('models/lr1e-05-warmup0.3')\n",
    "model.to(device)\n",
    "\n",
    "results_save_dir = 'predictions/'\n",
    "\n",
    "# Evaluate and save prediction results in each domain\n",
    "# Replace \"...\" with your code\n",
    "for domain in ['books', 'dvd', 'electronics', 'housewares']:\n",
    "    \n",
    "    test_dataset = SADataset(\"data/test_sa_\"+domain+\".jsonl\", tokenizer)\n",
    "    dev_loss, confusion, f1_pos, f1_neg = evaluate(test_dataset, model, device, batch_size=batch_size,\n",
    "                                                   result_save_file='predictions/test_'+domain+'.jsonl')\n",
    "    macro_f1 = (f1_pos + f1_neg) / 2\n",
    "\n",
    "    print(f'Domain: {domain}')\n",
    "    print(f'Validation Loss: {dev_loss:.3f}')\n",
    "    print(f'Confusion Matrix:')\n",
    "    print(confusion)\n",
    "    print(f'F1: ({f1_pos*100:.2f}%, {f1_neg*100:.2f}%) | Macro-F1: {macro_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO🔻: (2) [textual question] compare and discuss the results. \n",
    "\n",
    "**Questions:**\n",
    "- On which domain does the model perform the best? the worst?\n",
    "- Give some possible explanations of why the model's best-performed domain is easier, and why the model's worst-performed domain is more challenging. Use some examples to support your explanations.\n",
    "\n",
    "**Note:** To find examples for supporting your discussion, save the model prediction results on each domain under the `predictions/` folder, by specifying the `result_save_file` parameter in the *evaluate* function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model performs the best in houseware and the worst in dvd.**\n",
    "**This can be due to the fact that we use a larger specter to review movies compared to houseware and houseware reviews are geenrally concise and straight to the point in comparision to the movies reviews where we can find mixed feelings about several aspects.**\n",
    "{\"review\": \"The flight into New York had been a long one; but I was gently revived when Kirsten Dunst met me at the airport.  She was to be my escort that night at the Lincoln Center where the President would present me the Distinguished Writer's Cross-the highest award this nation can bestow on an author.  I quickly slipped my only suitcase in the backseat and sat next to her as she drove her Mercedes S class sedan to the Waldorf Astoria downtown. \\\"I have to admit that I didn't you who you were until I read STARLESS GRASSLANDS.  I don't mean to embarrass you; but that book changed my life.\\\" I tried to be polite and gracious toward the compliment.  For the remainder of the trip we had a nice conversation while I approved the fluid motion of her hands and legs as she maneuvered the controls of the automobile.  I very much approved of her legs. As I departed the car, Kirsten leaned toward the open door to catch my eyes. \\\"Don't forget.  I'll be by at seven so we can go to the ceremony together.  Please be ready.\\\" I expressed gratitude for her generosity and promised to be waiting for her seven sharp.  The hotel staff greeted me and soon I entered the elevator for a quiet ride to the eighty sixth floor.  As I entered the suite, the steward took my request to have my suit blushed, prepared and returned in a few hours. Finally, a bit of peace.  I poured a small measure of whiskey into a chilled tumbler and then walked over to the large window looking out over the city below.  For a few moments I contemplated the remarks I was to make at this evening's formal ceremonies, then I heard the hushed movement of stocking feet behind me across the room. I turned around and out of the bedroom to my left in walked Sandy Bullock dressed in a black sheer see through cat suit. \\\"Well, Crabby, aren't you glad to see me?\\\" Before I could recover from my surprise another set of feet wisped across the floor from the sitting room on the right.  It was Jennifer Aniston dressed in the same black sheet cat suit. \\\"Sweetheart, don't look at her.  Come here to me.\\\" \\\"Get out!\\\" screamed Sandy. \\\"No, you get out.  It's not fair.  I left Brad for him so I get him!\\\" The two of them stood in angry silence staring at each other for a tense moment.  Then Sandy leapt at Jennifer and instantly they were in a bitter fight hissing and scratching each other.  It was a ferocious sight as pulled hair, shrieks of pain, and nylon tearing could be heard.  Then almost a quickly as it began, the fighting pair disappeared into the sitting room.  Several sharp punches were heard then a definite thump as a body fell to the floor. Sandy marched back into the room victorious.  I ran to the door of the sitting room only to see Jennifer laid out on the floor out cold, spread eagle and naked.  Before I could think of what to do Sandy tapped me on my shoulder.  I turned around and there Sandy stood.  A little worse for wear.  Trying to catch her breath.  Nude with her torn cat suit draped across her feet. \\\"Forget her.  She won't bother us now.\\\" Sandy brushed the stray hairs out of her face, smiled and stepped out of her nylon costume.  She posed standing and anticipating my admiration. \\\"Now I'm all sweaty and warm.  I need a shower.\\\"  Sandy turned her lithe body and walked into the bedroom and into the bathroom beyond. I turned around and wondered what I should do for poor Jennifer still laying unconscious on the floor.  Then I heard Sandy calling. \\\"Crabby, why don't you come and help me?  You could soap up my......\\\" OOPS!!!!  Sorry!!!  Err...just a little fantasy of mine.  I meant to honestly review this movie but my more realistic nature took over.  I swear my pitiful fantasy is actually much better than the story told here.  You see, the cover of this DVD displays Ms. Bullock prominently with her name in big font letters.  But in truth this movie focuses on Tate Donovan as the main character and only on Ms. Bullock as his unlikely love interest. As much as I \\\"love\\\" Sandra Bullock, I have to admit her filmography has not been kind to her.  Her best films have been WHILE YOU WERE SLEEPING, WRESTLING ERNEST HEMINGWAY and the much-unappreciated HOPE FLOATS.  Unfortunately for each of these gems are several terrible movies like FIRE ON THE AMAZON, HANGMAN, SPEED 2 and this movie. The plot revolves around two \\\"losers in love\\\" who are unappealing both in looks and personality.  Donovan comes across a love potion from a gypsy woman (an unrecognizable Anne Bancroft) and soon both Donovan and Bullock have the most desirable members of the opposite sex at their feet.  All goes well until others find out their secret and Donovan and Bullock discover that their true love is actually between each other. The story is predictable and unfortunately not very funny or all that sweet.  Chances are you can find a copy of this DVD fairly cheap and it is not a bad way to kill some time while you're waiting for your own boyfriend or girlfriend to get off work-or your friends to wake up from their naps-so you can really do something worth wild.\", \"domain\": \"dvd\", \"label\": \"negative\", \"prediction\": \"positive\"} **This review invokes several aspects of a movie and describes his whole experience, in comparision to this review :** {\"review\": \"First the plastic top broke off at 6 months.  Then it stopped working at 9. I have one word: Junk\", \"domain\": \"housewares\", \"label\": \"negative\", \"prediction\": \"negative\"} **where we can see that it is straight to the point and direct.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyMZ5E4-QxM"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "# PART 2: Identify Model Shortcuts (22 pts)\n",
    "\n",
    "In this part, We aim to find out the shortcut features learnt by the sentiment analysis model we have trained in Part1. We will be using the model checkpoint trained with `learning rate=1e-5`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lCHLdaH_3vT"
   },
   "source": [
    "## 🎯 Q2.1: **N-gram Pattern Extraction (6 pts)**\n",
    "We hypothesize that `n-gram`s could be the potential shortcut features learnt by the SA model. An `n-gram` is defined as a sequence of n consecutive words appeared in a natural language sentence or paragraph. \n",
    "\n",
    "Thus, we aim to extract that an n-gram that appears in a review may serve as a key indicator of the polarity of the review's sentiment, for example:\n",
    "\n",
    ">- **Review 1**: This book was **horrible**. If it was possible to rate it **lower than one star** I would have.\n",
    ">- **Review 2**: **Excellent** book, **highly recommended**. Helps to put a realistic perspective on millionaires.\n",
    "\n",
    "For Review 1, the `1-gram \"horrible\"` and the `4-gram \"lower than one star\"` serve as two key indicators of negative sentiment. While for Review 2, the `1-gram \"excellent\"` and the `2-gram \"highly recommended\"` obviously indicate positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NovYRxv_3vU"
   },
   "source": [
    "TODO🔻: (1) [coding question] Complete `ngram_extraction()` function in `shortcut.py` file.\n",
    "\n",
    "The returned *ngrams* contains a **list** of dictionaries. The `n-th` **dictionary** corresponds the `n-grams` (n=1,2, 3, 4).\n",
    "\n",
    "The keys of each dictionary should be a **unique n-gram string** appeared in reviews, and the value of each n-gram key records the frequency of positive/negative predictions **made by the model** when the n-gram appears in the review, i.e., `\\[#positive_predictions, #negative_predictions\\]`.\n",
    "\n",
    "> Example: **`ngrams`[0]['horrible'][0]** should return the number of the positive predictions made by the model when the 1-gram token 'horrible' appear in the given review. i.e., \\[#positive_predictions, #negative_predictions\\].\n",
    "\n",
    "**Note:** (1) All the sequences contain punctuations should NOT be counted as a n-gram (e.g. `it is great .` is NOT a 4-gram, but `it is great` is a 3-gram); (2) All stop-words should NOT be counted as 1-grams, but can appear in other n-gram sequences (e.g. `is` is NOT a 1-gram token, but `it is great` can be a 3-gram token.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTHt1frZ_3vU"
   },
   "source": [
    "## 🎯 Q2.2: **Distill Potentially Useful Patterns (8 pts)**\n",
    "\n",
    "TODO🔻: (2) [coding question] For each group of n-grams (n=1,2,3,4), find and **print** the **top-100 n-gram sequences** with the **greatest frequency of appearance**, which could contain frequent semantic features and would be used as our feature list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IFZm6tFJFmlg"
   },
   "outputs": [],
   "source": [
    "from shortcut import ngram_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TUyal5mW_3vU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600/1600 [00:01<00:00, 927.43it/s] \n",
      "100%|██████████| 1600/1600 [00:01<00:00, 970.44it/s] \n",
      "100%|██████████| 1600/1600 [00:01<00:00, 1557.93it/s]\n",
      "100%|██████████| 1600/1600 [00:00<00:00, 2369.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 most frequent 1-grams:\n",
      "[(\"'s\", [3842, 3078]), (\"'t\", [2309, 2687]), ('one', [2143, 1844]), ('book', [1974, 1777]), ('great', [1347, 527]), ('like', [1293, 1269]), ('good', [1176, 954]), ('well', [1118, 601]), ('time', [1011, 896]), ('movie', [943, 978]), ('would', [932, 1286]), ('use', [925, 601]), ('get', [903, 1004]), ('also', [863, 521]), ('film', [789, 631]), ('j', [766, 514]), ('much', [752, 751]), ('even', [722, 870]), ('first', [719, 687]), ('read', [715, 645]), ('really', [712, 700]), ('k', [687, 373]), ('love', [685, 262]), ('many', [639, 459]), (\"'ve\", [618, 473]), (').', [601, 461]), ('best', [601, 260]), ('l', [597, 381]), ('way', [585, 561]), ('little', [579, 394]), ('b', [572, 453]), ('new', [571, 465]), ('work', [562, 583]), ('people', [544, 478]), ('see', [543, 414]), ('two', [540, 494]), ('r', [537, 411]), ('...', [535, 641]), ('g', [533, 404]), ('man', [532, 386]), ('us', [528, 339]), ('vd', [526, 422]), ('story', [520, 442]), ('better', [507, 559]), ('make', [507, 505]), ('2', [502, 517]), ('w', [501, 364]), ('easy', [495, 120]), ('c', [491, 352]), ('still', [486, 373]), ('h', [478, 402]), (\"'m\", [476, 528]), ('could', [474, 590]), ('never', [456, 419]), ('p', [456, 372]), ('years', [453, 384]), ('life', [448, 319]), ('set', [447, 313]), ('used', [443, 367]), ('quality', [442, 313]), ('know', [430, 432]), ('want', [427, 355]), ('n', [418, 278]), ('think', [417, 489]), ('sound', [415, 211]), ('back', [413, 558]), ('3', [410, 393]), ('--', [400, 394]), ('need', [400, 229]), ('buy', [398, 506]), ('go', [397, 366]), ('ing', [390, 322]), ('e', [388, 318]), ('works', [386, 200]), ('find', [383, 352]), ('made', [382, 383]), ('),', [380, 217]), ('right', [375, 273]), ('ever', [372, 310]), ('bought', [369, 438]), ('v', [366, 265]), ('every', [358, 309]), ('old', [358, 295]), ('another', [356, 403]), ('world', [356, 205]), ('1', [355, 357]), ('price', [353, 173]), ('say', [347, 335]), ('able', [347, 316]), ('makes', [343, 209]), ('got', [342, 401]), ('since', [342, 294]), ('recommend', [338, 188]), ('long', [337, 270]), ('far', [337, 241]), ('f', [335, 278]), ('5', [330, 295]), ('product', [326, 437]), ('end', [326, 350]), ('er', [326, 299])]\n",
      "Top-100 most frequent 2-grams:\n",
      "[('of the', [2827, 2170]), ('in the', [1645, 1377]), ('is a', [1219, 672]), (\"it 's\", [1088, 880]), ('and the', [1035, 808]), ('it is', [983, 707]), ('on the', [971, 842]), ('this is', [931, 648]), ('to the', [930, 793]), ('i have', [884, 719]), ('if you', [877, 722]), ('this book', [791, 734]), ('for the', [763, 541]), ('to be', [762, 756]), (\"don 't\", [700, 828]), ('with the', [697, 583]), ('and i', [624, 572]), ('in a', [561, 428]), ('is the', [561, 371]), ('d vd', [524, 422]), ('the book', [520, 488]), ('one of', [488, 302]), ('i was', [485, 642]), (\"i 've\", [481, 352]), (\"i 'm\", [467, 517]), ('it was', [456, 588]), ('i am', [453, 437]), ('from the', [453, 382]), ('for a', [439, 444]), ('with a', [436, 302]), ('a great', [435, 138]), ('as a', [422, 386]), ('and it', [419, 367]), ('of a', [417, 386]), ('you can', [399, 232]), ('at the', [394, 356]), ('that i', [378, 340]), ('that the', [377, 369]), ('the best', [376, 123]), ('all the', [341, 277]), ('is not', [335, 403]), ('but i', [331, 362]), ('i can', [328, 286]), ('as the', [326, 219]), ('in this', [318, 300]), ('they are', [313, 257]), ('there is', [311, 356]), ('easy to', [308, 67]), ('this movie', [306, 332]), ('have to', [303, 338]), ('a good', [302, 261]), (\"can 't\", [301, 272]), ('the same', [299, 366]), ('i would', [299, 364]), ('the first', [298, 345]), ('of this', [297, 300]), ('a little', [297, 145]), (\"doesn 't\", [296, 389]), ('to get', [294, 330]), ('a lot', [292, 217]), ('the movie', [290, 276]), ('but it', [287, 272]), ('and a', [284, 198]), ('to use', [279, 177]), ('have a', [275, 230]), ('the film', [273, 207]), ('about the', [272, 209]), ('as well', [271, 119]), ('there are', [269, 235]), ('you are', [269, 194]), ('i had', [267, 370]), ('was a', [264, 297]), ('want to', [262, 205]), ('that is', [257, 232]), ('when i', [256, 321]), ('this one', [256, 265]), ('i love', [256, 76]), ('a few', [251, 220]), (\"didn 't\", [246, 351]), ('is that', [245, 215]), ('in my', [245, 191]), ('into the', [244, 155]), ('out of', [243, 300]), ('i bought', [243, 297]), ('on a', [243, 169]), ('a very', [240, 136]), ('i don', [238, 265]), ('by the', [238, 250]), (\"'s a\", [238, 161]), ('have been', [235, 292]), ('qu ot', [235, 171]), ('the most', [228, 164]), ('so i', [226, 246]), ('the only', [226, 209]), ('has a', [226, 152]), ('book is', [224, 206]), ('a bit', [223, 120]), ('to a', [222, 182]), ('some of', [222, 156]), ('the story', [220, 193])]\n",
      "Top-100 most frequent 3-grams:\n",
      "[('this is a', [303, 142]), ('one of the', [282, 166]), (\"i don 't\", [237, 265]), ('a lot of', [193, 153]), (\"it 's a\", [165, 100]), ('if you are', [150, 103]), ('some of the', [150, 92]), ('it is a', [149, 74]), ('this d vd', [145, 126]), ('this is the', [141, 84]), ('as well as', [131, 48]), ('is a great', [131, 26]), ('this book is', [122, 120]), (\"i can 't\", [119, 115]), (\"you don 't\", [107, 69]), ('is one of', [107, 50]), ('to be a', [101, 95]), ('the d vd', [101, 85]), (\"i didn 't\", [100, 109]), ('i bought this', [99, 135]), ('if you want', [93, 88]), ('the fact that', [88, 97]), (\"if you 're\", [88, 79]), ('of the best', [87, 13]), ('easy to use', [84, 12]), ('out of the', [80, 91]), (\"it 's not\", [79, 94]), ('the book is', [79, 67]), ('there is a', [78, 70]), ('you want to', [76, 63]), (\"i 'm not\", [75, 97]), ('most of the', [75, 81]), (\"and it 's\", [75, 44]), ('of the book', [74, 87]), ('if you have', [74, 60]), ('i highly recommend', [74, 6]), (\"it doesn 't\", [73, 92]), ('i have to', [71, 103]), ('there is no', [70, 100]), ('all of the', [69, 70]), (\"don 't have\", [69, 41]), ('in the book', [68, 57]), ('i have been', [68, 38]), ('is the best', [68, 13]), (\"but it 's\", [66, 47]), ('of the most', [65, 35]), (\"don 't know\", [64, 81]), (\"you can 't\", [63, 45]), ('at the same', [63, 19]), ('you have to', [61, 92]), ('of this book', [61, 57]), ('in this book', [60, 79]), ('be able to', [60, 46]), ('part of the', [59, 42]), ('j es us', [58, 24]), ('easy to clean', [58, 6]), ('the rest of', [56, 56]), ('i love the', [56, 25]), ('it has a', [55, 26]), ('is a very', [55, 22]), ('the same time', [55, 21]), ('highly recommend this', [55, 1]), ('i had to', [54, 104]), ('it was a', [54, 78]), ('a couple of', [54, 57]), ('is that the', [54, 37]), ('the ip od', [54, 37]), ('d vd is', [54, 35]), ('d v ds', [54, 29]), ('i would recommend', [54, 25]), ('and it is', [53, 47]), ('but it is', [53, 37]), ('i have used', [53, 21]), (\"they don 't\", [52, 50]), ('of the film', [52, 50]), ('and i have', [52, 48]), ('this is an', [52, 14]), ('the first time', [51, 57]), (\"i 've ever\", [51, 40]), (\"i haven 't\", [51, 30]), ('read this book', [51, 29]), ('new y ork', [51, 27]), ('a bit of', [51, 26]), ('i love this', [51, 4]), ('this movie is', [50, 63]), ('looking for a', [50, 37]), ('it would be', [49, 62]), ('seems to be', [49, 44]), ('all in all', [49, 27]), ('very easy to', [49, 12]), (\"'t want to\", [48, 33]), ('i am very', [48, 23]), (\"you won 't\", [48, 15]), ('for the price', [48, 14]), ('i am a', [47, 52]), (\"i 've had\", [47, 46]), ('i have had', [47, 39]), ('the dish washer', [47, 25]), (\"i couldn 't\", [46, 66]), ('in order to', [46, 34])]\n",
      "Top-100 most frequent 4-grams:\n",
      "[('this is a great', [70, 8]), ('one of the best', [66, 10]), ('is one of the', [61, 34]), ('if you want to', [54, 41]), ('at the same time', [53, 19]), ('one of the most', [48, 28]), (\"i don 't know\", [44, 57]), (\"if you don 't\", [41, 28]), ('the rest of the', [39, 36]), ('this is the best', [35, 3]), ('i highly recommend this', [35, 1]), ('this is one of', [34, 10]), ('this d vd is', [33, 19]), (\"you don 't have\", [30, 6]), (\"i don 't think\", [29, 39]), ('in the dish washer', [29, 14]), (\"don 't want to\", [28, 19]), ('to be able to', [28, 19]), ('as well as the', [28, 15]), ('for the first time', [27, 13]), (\"it 's a great\", [27, 5]), ('the quality of the', [25, 16]), ('the end of the', [24, 41]), ('if you are a', [24, 19]), ('this is a very', [24, 8]), ('i would recommend this', [24, 2]), ('if you are looking', [22, 16]), ('easy to use and', [22, 5]), ('i was looking for', [21, 14]), ('this book is a', [21, 8]), ('i would highly recommend', [21, 4]), (\"you won 't be\", [21, 2]), ('when it comes to', [20, 24]), ('you are looking for', [20, 18]), ('if you have a', [20, 14]), (\"if you haven 't\", [20, 3]), ('recommend this book to', [20, 3]), ('this is an excellent', [20, 0]), ('at the end of', [19, 26]), (\"and i don 't\", [19, 16]), ('i am very pleased', [19, 0]), ('this is a must', [19, 0]), (\"i 'm going to\", [18, 34]), ('the bottom of the', [18, 18]), ('out of the box', [18, 16]), ('the j apan ese', [18, 13]), (\"don 't have to\", [18, 6]), ('highly recommend this book', [18, 0]), (\"i 'm not sure\", [17, 19]), ('one of my favorite', [17, 5]), ('k uro s awa', [17, 2]), ('this is not a', [16, 20]), (\"but i don 't\", [16, 19]), (\"i 've ever seen\", [16, 17]), ('was one of the', [16, 14]), (\"i didn 't like\", [16, 12]), ('i have ever seen', [16, 12]), ('for a long time', [16, 8]), (\"you don 't want\", [16, 6]), ('k l ips ch', [16, 4]), (\"if you 're looking\", [15, 11]), ('i have to admit', [15, 7]), ('it is easy to', [15, 6]), ('on the d vd', [15, 6]), ('is one of my', [15, 5]), (\"i haven 't had\", [15, 5]), ('is very easy to', [15, 4]), ('it is a great', [15, 3]), ('does a great job', [15, 1]), ('i highly recommend it', [15, 0]), (\"i don 't have\", [14, 16]), (\"don 't know what\", [14, 14]), ('i was able to', [14, 14]), ('the fact that it', [14, 13]), ('over and over again', [14, 11]), ('a bit of a', [14, 8]), ('the sound quality is', [14, 8]), ('is one of those', [14, 6]), ('as far as i', [14, 4]), ('i have used it', [14, 3]), ('very easy to use', [14, 2]), ('ch ris ben oit', [14, 1]), ('am very happy with', [14, 1]), ('easy to clean and', [14, 1]), ('this is a wonderful', [14, 0]), ('j up itor hollow', [14, 0]), ('if you want a', [13, 23]), ('the back of the', [13, 14]), (\"you 're looking for\", [13, 12]), ('i read this book', [13, 10]), (\"don 't know if\", [13, 10]), ('the only thing i', [13, 9]), ('he nc ke ls', [13, 8]), ('the cal phal on', [13, 8]), ('are looking for a', [13, 7]), ('does a good job', [13, 7]), (\"you don 't need\", [13, 4]), (\"i can 't wait\", [13, 4]), ('to be the best', [13, 1]), ('i am very happy', [13, 1])]\n"
     ]
    }
   ],
   "source": [
    "# all your saved model prediction results from 1.3 Fine-Grained Validation\n",
    "prediction_files = ['predictions/test_books.jsonl', 'predictions/test_dvd.jsonl', 'predictions/test_electronics.jsonl', 'predictions/test_housewares.jsonl']\n",
    "\n",
    "# TODO: Define your tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "ngrams = ngram_extraction(prediction_files, tokenizer)\n",
    "\n",
    "top_100 = {}\n",
    "for n, counts in enumerate(ngrams):\n",
    "    # TODO: find top-100 n-grams (n=1,2,3 or 4) associated with the greatest frequency of appearance\n",
    "    top_100_freq = sorted(counts.items(), key=lambda x: x[1], reverse=True)[:100]\n",
    "\n",
    "    print(f'Top-100 most frequent {n+1}-grams:')\n",
    "    print(top_100_freq)\n",
    "\n",
    "    top_100[n] = top_100_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pz25EvuI_3vU"
   },
   "source": [
    "**Among each type of top-100 frequent n-grams above**, we aim to further find out the n-grams which **most likely** lead to *positive*/*negative* predictions (positive/negative shortcut features). \n",
    "\n",
    "TODO🔻: (3) [coding&text question] Design **two different methods to re-rank** the top-100 n-grams to extract shortcut features. For each method, you should extract **1** feature in each of n-grams group (n=1, 2, 3, 4) for positve and negative prediction (1\\*4\\*2=8 features in total for 1 method).\n",
    "\n",
    "Explain each of your design choices in natural language, and compare which method finds more reasonable patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD1** The method rank n-grams based on their positivity by computing the ratio of positive counts to negative counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 most positive 1-grams:\n",
      "[('easy', (496, 121))]\n",
      "Top-1 most positive 2-grams:\n",
      "[('easy to', (309, 68))]\n",
      "Top-1 most positive 3-grams:\n",
      "[('highly recommend this', (56, 2))]\n",
      "Top-1 most positive 4-grams:\n",
      "[('this is an excellent', (21, 1))]\n",
      "Top-1 most negative 1-grams:\n",
      "[('would', (933, 1287))]\n",
      "Top-1 most negative 2-grams:\n",
      "[(\"didn 't\", (247, 352))]\n",
      "Top-1 most negative 3-grams:\n",
      "[('i had to', (55, 105))]\n",
      "Top-1 most negative 4-grams:\n",
      "[(\"i 'm going to\", (19, 35))]\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "\n",
    "def positive_negative_words_ratio(top_100):\n",
    "    top_100_reranked = {}\n",
    "    top_100_tmp = top_100.copy()\n",
    "    for n, counts in top_100_tmp.items():\n",
    "        # to avoid division by zero add 1 to every count\n",
    "        counts = [(ngram, (pos + 1, neg + 1)) for ngram, (pos, neg) in counts]\n",
    "        top_100_reranked[n] = sorted(counts, key=lambda x: x[1][0] / x[1][1], reverse=True)\n",
    "\n",
    "    return top_100_reranked\n",
    "\n",
    "\n",
    "top_100_reranked1 = positive_negative_words_ratio(top_100)\n",
    "for n, counts in top_100_reranked1.items():\n",
    "    print(f'Top-1 most positive {n+1}-grams:')\n",
    "    print(counts[:1])\n",
    "\n",
    "for n, counts in top_100_reranked1.items():\n",
    "    print(f'Top-1 most negative {n+1}-grams:')\n",
    "    print(counts[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**METHOD2** This method balances the positivity by subtracting the negativities from the positive counts so that the ranking consider more nuanced cases where an n-gram has slightly fewer positive sentiments but significantly fewer negative sentiments, scoring higher than if they had many positives but also many negatives.\n",
    "The exponential denominator moderates against very high sentiment counts, balancing out so that n-grams with large counts of sentiments don't overly weigh up the rankings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Yq2cVOaWTEYw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 most positive 1-grams:\n",
      "[('price', (353, 173))]\n",
      "Top-1 most positive 2-grams:\n",
      "[('i love', (256, 76))]\n",
      "Top-1 most positive 3-grams:\n",
      "[('i love this', (51, 4))]\n",
      "Top-1 most positive 4-grams:\n",
      "[('this is a wonderful', (14, 0))]\n",
      "Top-1 most negative 1-grams:\n",
      "[('end', (326, 350))]\n",
      "Top-1 most negative 2-grams:\n",
      "[('so i', (226, 246))]\n",
      "Top-1 most negative 3-grams:\n",
      "[('i am a', (47, 52))]\n",
      "Top-1 most negative 4-grams:\n",
      "[('the back of the', (13, 14))]\n"
     ]
    }
   ],
   "source": [
    "# Method 2 \n",
    "import math\n",
    "def pos_neg_nuanced(top_100):\n",
    "    top_100_reranked = {}\n",
    "    top_100_tmp = top_100.copy()\n",
    "    \n",
    "    for n, counts in top_100_tmp.items():\n",
    "        # divide pos by exponential negative\n",
    "        counts = [(ngram, (pos, neg)) for ngram, (pos, neg) in counts]\n",
    "        top_100_reranked[n] = sorted(counts, key=lambda x: (x[1][0]-x[1][1]) / 2**(x[1][0] + x[1][1])  , reverse=True)\n",
    "\n",
    "    return top_100_reranked\n",
    "\n",
    "top_100_reranked2 = pos_neg_nuanced(top_100)\n",
    "for n, counts in top_100_reranked2.items():\n",
    "    print(f'Top-1 most positive {n+1}-grams:')\n",
    "    print(counts[:1])\n",
    "for n, counts in top_100_reranked2.items():\n",
    "    print(f'Top-1 most negative {n+1}-grams:')\n",
    "    print(counts[-1:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvKyF0UFuXXM"
   },
   "source": [
    "TODO🔻: Compare and discuss the results from two methods above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first method**\n",
    "\n",
    "It is clear that the first method performs good since the features extracted by this method are significant in the sentiment of a review. As we can see words like \"easy\", \"recommend\" and \"excellent\" show appreciation and are always in positive reviews. Also, \"had to\", \"need to\" and \"'t\" show frustration and are always present in negative reviews.\n",
    "\n",
    "**second method**\n",
    "\n",
    "Method 2 performs very well in the postive ngrams since 'i love' , 'i love this', 'this is a wonderful' are one of the most reliable indicators of a positive review. But in the negative ngrams performs very poor. \n",
    "\n",
    "**OVERALL COMPARISION** \n",
    "\n",
    "Eventhough, the second method performed better in the positive part, the first one is the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjVti-vL_3vV"
   },
   "source": [
    "## 🎯 Q2.3: **Case Study (8 pts)**\n",
    "\n",
    "TODO🔻: Among the shortcut features you found in 2.1, find out **4 representative** cases (pair of `\\[review, n-gram feature\\]`) where the shortcut feature **will lead to a wrong prediction**. \n",
    "\n",
    "For example, the 1-gram feature \"excellent\" has been considered as a shortcut for *positive* sentiment, while the ground-truth label of the given review containing \"excellent\" is *negative*.\n",
    "\n",
    "**Questions:**\n",
    "- Based on your case study, do you detect any limitations of the n-gram patterns?\n",
    "- Which type of n-gram (1/2/3/4-gram) pattern is more robust to be used for sentiment prediction shortcut and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2XyPmb01_3vV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : would |Positive : Having read Clive Cussler adventures for over 10 years, i was totally disgusted with the redneck and narrow minded attitudes expressed in the book. The father, who does not stay with his son, decides to revenge his son fighting a war trying to kill people in someone else's country. That is perfectly justified! The book mentions Hindu mercenaries who have not been seen anywhere in the world in any century much less this one. This is a direct insult to all Hindus as being one I am astonished at the insensitivity of the author. How come \"Hickman\" is not a \"Christian\" mercenary or for that matter the Corporation as they seem to be in this for the money. The justifications are ridiculous. India and Hindus of India have been terrorized, jailed, colonized and robbed by Christian and Muslims alike over the last 1000 plus years. You don't find us calling people by religion and we have all major religions in our secular country. We have grown spiritually to accept everyone, and I would like the authors to read more than a 2 page synopsis on religions before writing books. The book smacks of jingoism and is a pathetic display of the authors minimal knowledge of religions and global affairs |Negative :\n",
      "word : end |Positive : I agree with one the comments posted below. The main problem with this book is Mitch is a totally one-dimensional character and it's impossible to find any depth in him. Over the course of the book he doesn't change, he's unsympathetic, and by the time we've reached page 500, we don't know anything more about him than we did on page 10. I found myself dying to get to the end of this book, just so I could start another book with some substance and some well-drawn characters. Another major problem is that most of the characters in this book use the same wise-cracking speaking style so there is no sense of the characters being different from each other in any way. Overall, I thought this was mediocre and a bit of a waste of time |Negative :\n",
      "word : easy |Negative : \"OF PARADISE AND POWER\" - BY ROBERT KAGAN \"Americans are from Mars and Europeans are from Venus\" - apart from simply raising a few eyebrows, this line by Robert Kagan author of \"Of Paradise And Power\", has not only caught the attention of many but managed to stir up a wave of discussions among the politicians and elites concerned with cross-Atlantic relations. Kagan's thought provoking book became extremely popular and received international acclaim for its analysis of the deteriorating US-Europe relationship, not to mention its catchy and striking phrases systematically introduced throughout the slender yet captivating book. Kagan introduces multiple chapters in his book, in order to deconstruct the key underlying issues which he claims are causes of the ever widening divide between US and Europe. In the chapter \"The Power Gap\", Kagan starts from World War I to understand the aversiveness Europe displays towards military and wars. He argues that while both the World Wars weakened Europe tremendously; they helped the US emerge as a super power. He further elaborates on the might of the US in the chapter that follows - \"Psychologies of Power and Weakness\". Kagan explains how lack of power can not only affect psychologies and ideologies, but change perspectives as well. He claims Europe's paucity of military power compels it to not only tolerate threats but ignore them all together and at the same time try and counterbalance the US force by constructing a world which is governed by economic and soft non-violent methods. This chapter culminates into \"Hyperpussiance\", a chapter in which Kagan discusses intricate details of Kosovo and Bosnia, highlighting Europe's failure and America's effectiveness in handling the situation. He finally presents his central argument in the chapters \"The Post Modern Paradise\" and \"The World America Made\" where Kagan states, \"Europe's evolution into its present state occurred under the mantle of the U.S. security guarantee and could not have occurred without it.\" Casting doubts on Europe's intentions, Kagan centers his argument on the growing self-centered policies of Europe and questions the validity of cohesive terms such as `west' in his chapter \"Is It Still `The West'\"? Finally, it seems Kagan provides a solution to this insolvable problem of the US- Europe divide in his concluding chapter, \"Adjusting to Hegemony\", by simply stating one line - \"the task for both Europeans and Americans is to readjust to the new reality of American hegemony\". This extremely small book, as compared to the issue it addresses needs further analysis and some close examining. There are many issues to be addressed in Kagan's brief analysis and one might not be completely satisfied with certain claims, reasonings or the manner in which such a grave and sensitive topic is handled. While there is a tone of underlying bias (in contrast to the general opinion of a well-balanced view that the author holds), Kagan manages to miss out some crucial facts and figures, not to mention present stark paradoxes, and the essay lacks sufficient citings. To start with, let me point out the contradictions in Kagan's arguments, the presence of which highly undermines his claims, leaving the reader in a dilemma unconvinced of either argument. Kagan claims, \"Europe's relative weakness has understandably produced a powerful European interest in building a world where military strength and hard power matter less than economic and soft power...rules of behavior\" (pg.37). He goes on to say, - \"Since Europeans lack the capacity to undertake unilateral military actions, either individually or collectively as `Europe,' it is natural that they should oppose allowing others to do that they cannot do themselves.\"(pg.38) in contrast to \"...Europe today has the wealth and technological capability to make itself more of a world power in military terms if Europe wanted to become that kind of world power\" (pg.53-54). Kagan contradicts his own views, stating many times that Europe is militarily weak, nor does it have the capacity to build up its forces yet at the same time, puts forth an argument of Europe's capabilities of increasing its defense spending and military might to match the US. In the chapter, Psychologies of Power and Weakness, Kagan's one claim contradicts another which is made in the next chapter as the reasoning for both his claims seem to be absolutely tangential. He first explains Europe's greater tolerance for threats is due to its weakness and the fearful past, which it has suffered from. He then metaphorically explains it by giving the example of a man with a knife and the same one with a rifle having different interpretations of the word `threat' when confronting a bear (pg.31). But again Kagan himself feels Europe can become a military power as and when it wishes to, when he says \"They could easily spend twice as much as they are currently spending on defense if they believed it necessary to do so\" (pg.54). So this forces me to ask a question, would one be mauled by a bear if one could produce a rifle to prevent this misfortune? So isn't it quite possible that Europeans feel safe, not because they \"...enjoy the `free ride' they have gotten under the American security umbrella over the past six decades\" (pg.54), but rather because there was and is no real threat at all? With the use of phrases such as \"The Axis of Evil\" and \"Rouge States\" (pg.30), which Kagan uses to describe Iraq and Iran, not to mention his references to `Weapons of Mass Destruction' (which he uses as a pretext to the war in Iraq), which still seem to be elusive for some reason, he seems to perplex me by blatantly ignoring his own creation. While Kagan maintains that Iraq was a threat and the US perceived it as one while Europe did not, he might want to explain as to why, in 1997, he (Kagan was the Director of the Project for the New American Century) signed a document which justifies American occupation of the Middle-East, irrespective of threats from Iraq or Iran. \"The United States has for decades sought to play a more permanent role in Gulf regional security. While the unresolved conflict with Iraq provides the immediate justification, the need for a substantial American force presence in the Gulf transcends the issue of the regime of Saddam Hussein... From an American perspective, the value of such bases would endure even should Saddam pass from the scene. Over the long term, Iran may well prove as large a threat to U.S. interests in the Gulf as Iraq has. And even should U.S.-Iranian relations improve; retaining forward-based forces in the region would still be an essential element in U.S. security strategy given the longstanding American interests in the region.\"(pg.17 - Rebuilding America's Defenses - *1) This forces me to conclude that either the author has resorted to `cherry picking' of evidence or he suffers from selective amnesia. Some of his reasoning, analysis and analogies seem to be too na\u001ave. When explaining military action, one cannot justify it by saying, \"When you have a hammer, all problems start to look like nails\". The author speaks about the nation's (US's) willingness to go to war simply because it has the ability and the capacity to do so. Without discussing a need or providing concrete reasons for the use of aggression, the author seems to be content in justifying use of force, simply because one has it. One important aspect Kagan leaves out in his analysis are the political and economical aspects of the ongoing US-Europe feud. While I have discussed some economical aspects later in the review, one must note that Kagan does not mention the Kyoto Protocol (apart from Clinton's negotiation of it (pg.45)). The US withdrawal from the Protocol had created serious problems and divisions between the two giants (www.climnet.org - *2). While there were many other issues that further fanned the growing fire, disagreements over the Anti-Ballistic Missile treaty for example, which Kagan conveniently sweeps under the carpet, needed to be addressed if one is discussing transatlantic relations (www.bbcnews.com - *3, 4) For a book which covers issues from the pre World War II decade (occasionally going back to ancient philosophy) to the present issues, 72 citations, from which many are simply extended explanations (for examples, see supra notes- 19, 20, 22), seem to be alarmingly low, especially with the absence of some important ones and not to mention, certain citations which wrongly allude to historical facts. Kagan mentions, \"As `some' Europeans put it, the real division of labor consisted of the United States making the dinner and the Europeans doing the dishes\" (pg.23), yet he does not refer to any source for this cynical statement. He says, \"The vast majority of Europeans always believed that the threat (if any) posed by Saddam was more tolerable than the risk of removing him\" along with other alleging statements like, \"The rehabilitation and reintegration of Saddam Hussein's Iraq is precisely what they(Europe) sought\" (pg.44) , and again gives no source for the origin of these beliefs. Strangely, he supports one of his claims by stating in his supra note.20, pg.30, \"for that matter, this is also the view commonly found in American textbooks\". One might not generally expect to see such puerile purporting evidence in a scholarly analysis of transatlantic relations. Kagan at times jumps back to Athenian times and has injected strong philosophies in his essay. He mentions a number of times how Americans see the world from a Hobbesian perspective while Europeans are followers of Kantinian policies. Kagan seems to have overlooked the fact, that though Hobbes had described the state of nature as that of \"all against all\", the only reason Hobbes justifies a sovereignty is to ensure peace on earth. Kagan conveniently cites Hobbes (Hobbes theory itself was flawed as later philosophers termed it as `Hobbes Dilemma') to justify American aggression, but no where in Hobbes' theory is aggression justified by the sovereign (Leviathan - Ch.17). There are some conclusions Kagan draws which seem to be beliefs of an American idealist living in a world where politics is as pure as religion. He concludes that while the US was providing \"free security\" to Europe after the Cold War, \"...America's great power and willingness to assume responsibility for protecting other nations...\" (pg.34) has given Europe the opportunity to build up economically. Kagan further asserts his point when he says, \"Given America's willingness to spend so much money protecting them, Europeans would rather spend their own money on social welfare programs, long vacations and shorter workweeks\" (pg.54). These are inaccurate presentations of facts, which are more of subjective, bias ideologies. I would like to put forth two points in order to accentuate the conspicuous fallacy of these two statements. First, America's interest in Europe was not to provide it with security whether before or after the Cold War. One must not confuse the fact that measures taken to restrict the Soviet advance was purely for selfish motives, rather than for the benefit of nations which were under the watchful eye of the communist empire (Kagan himself acknowledges the fact when he says, \"With the check of the Soviet power removed, the United States was free to intervene practically whenever and wherever it chose...\"(pg26)). Second, when Kagan acknowledges that Europe which has a \"$9 trillion \" economy is in a comfortable position to support its welfare programs along with increased military spending; attributing the fact that Europe spends more on welfare, as he calls them \"free-riders\", results in a faulty analysis. When I had mentioned earlier that Kagan chose to omit and ignore certain facts, the example of the Balkans, which the author has cited many times in his book would seem most relevant. Kosovo is one of the primary issues he discusses to bring forth US benevolence and European incapacity. \"American involvement in Kosovo or Bosnia was not based on calculations of a narrow American `national interest'...While Americans had a compelling moral interest in stopping genocide and ethnic cleansing,... the US had no `national interest' at stake in the Balkans\"(pg.50), this is how Kagan sees it. Indeed stopping genocide was a concern, but was it the only concern? While Milosevic had very much agreed to station NATO troops in Kosovo to stop the conflicts, none seem to have been interested in his offer; they rather wanted complete access and control to all of Yugoslavia. The Rambouillet Accord, to which the Contact Group added an Appendix B on the last day of the conference held at Paris, one must note was the primary reason for rejecting NATO troops, as it demanded Yugoslavia to surrender its autonomy and sovereignty. Yet, he does not mention of Milosevic agreeing to station NATO troops in Kosovo (http://wsws.org *5). Some thing missing from Kagan's entire book are the words \"Latin America\" and \"Africa\". As seen above, Kagan believes America was spending its money protecting Europe and Europe alone from all the threats around the world, before and after the Cold War. But what baffles me to quite an extent is that not once does Kagan mention about any American intervention in Latin America (official or not), Africa or the Far East for that matter. Those who are well aware of the world around them, will be aware of America's support for dictators and other corrupt regimes throughout Latin America and the far East, Indonesia for example, which were for self serving purposes alone or is it that the author is implying that the Latin American and African countries were a threat to Europe, hence the interventions and no mention of them? (A complete timeline by Steven Kangas can be found on multiple sites *6, 7, 8) There is one severe `defect', as I put it, which undermines the entire argument of this book. Kagan's tunneled vision approach, which refers to the word `power' only in terms of military strength and capabilities, ultimately results in a false dichotomy, which is the stumbling stone for his thesis. Kagan believes the world and in particular, Europe, is left with two options, that either they follow the US or be a tacit audience. The reason he reaches this conclusion as I previously put it, is due to his convergent view. \"Rather than viewing the US as a Gulliver tied down by Lilliputian threads\", says Kagan, \"American leaders should realize that they are hardly constrained at all, that Europe is not really capable of constraining the US\". He reinforces this claim throughout his book (see pg- 33, 37-40, 59, 62, etc). The author's abysmal ignorance about hard-hitting, highly valid and relevant facts has rendered severe blows to the validity of his claims. Kagan has committed serious errors here, by ignoring Europe's strength, not military but political and economical indeed. With a population of 456,953,258 (the CIA Fact Book *9) and still growing, the EU, leave aside Europe, cannot be simply overlooked or brushed aside as a dead competitor or a bygone culture. Europe is economically strong enough to trouble the US as and when it wants to, the recent problems over the genetically altered meat, banana dispute, steel industry debacle, Airbus-Boeing clash and many more, have all proved that Europe's economic strength is much of a concern for the US.  While I must point out a rank deletion, I believe this is `the' single factor which is capable for arguing against all of Kagan's thesis and arguments, and that is the American Economy. Not once has he mentioned the deteriorating state of American economy (2001), even though problems did exist when he wrote this book. He seems to be blissfully ignorant and persists that America can sustain its exorbitant defense spending, \"...the US can sustain its current military spending levels and its current global dominance far into the future\". Strangely, the growing inflation rate, rising unemployment, stagflation, trends the economy was showing of heading towards a recession, were I believe dismissed as myths by the author (for statistics see - www.bbcnews.com; http://wsws.org *10, 11). Also, Kagan's final solution suggest that, \"The obvious answer is that Europe should.... and build up its military, even if only marginally\", leaves me with a conundrum. Is Kagan implicating that Europe has no military at all or is it that he wants to begin a new arms race, this time Cold War II? Kagan must understand he is writing this book for international publics and not simply those, whose ideologies are in tandem with his. Hence, I doubt most would disagree that the world should get into an arms race here, as we all know, there is no stopping to this evil if it once starts. While I have already discussed the effects of trade and economics (which Kagan ignores) on the US-Europe relation, there is something I must point out before I end this review. Kagan, rather conveniently digresses of the topic, which is transatlantic relationships, to justifying US hegemony in a neo-conservative fashion. While most Americans themselves would probably not agree with Kagan's ideology (as the public opinion and faith plummets - see polls from various news sources *12), leave aside Europeans or rest of the world, Kagan writes almost 15 pages to justify it. This was a rather disappointing conclusion to his book, where he thinks the only solution to the ever widening divide is \"Adjusting to Hegemony\". When Kagan says how many (statistics seem to abandon him) Americans believe that by advancing their interest, they advance the interest of humanity, he sounds exuberantly jingoistic and to drive the point home, he quotes Benjamin Franklin as saying - \"America's cause is the cause of all mankind\" and again I must bring to the author's notice, the recipient of these remarks is an international audience, which is diverse, has its own culture and heritage, religious beliefs and ideologies and has no reason to believe why their culture or beliefs are inferior to any other.  An author of his stature must realize that it is transatlantic relationships and not a primary grade bully that is being discussed. While he seems to take pride in saying that America has a \"go it alone\" attitude (pg.39, 99), it does not abide by the UN Security Council (pg.40, 99), takes action unilaterally irrespective of international laws (pg.45, 61, 99), is justified in adopting `double standards' (pg.62, 99), he fails to fathom the gravity of the current day (or rather the day when he wrote this book) situation and the growing unpopularity of American foreign policy around the world. While he sees Europe as an obstacle in the path of America's imperialistic goals, a neutral observer would be more than justified in pointing out that may be it is the US that hinders progress and efforts of the EU to establish a world which respects international laws, human rights, collective public opinion and most of all - peace. He fails to give an impartial, if not a complete solution to rid the world of this wedge as he weakly concludes his essay by saying, \"...a little common understanding can still go a long way\". While many have nothing but words of praise, as a one line summary by Francis Fukuyama calls it - \"Brilliant\" (a co-architect and signatory of the Rebuilding America's Defense Project) and according to Dr. Henry Kissinger, [against whom innumerable charges of human right violations have been levied, who ironically, Kagan terms as a \"quintessential realist\". See - http://en.wikipedia.org(for cases) *13; http://www.thenation.com(for list of crimes) *14] the book is a \"seminal treatise\" which would \"shape the (US-Europe relation) discussion for years to come\"; I do not particularly endorse it it. As I have already put forth immense supportive data to show why the book lacks a concrete argument, sane reasoning or supportive evidence, though its lucid prose, easy comprehension and its articulate linking makes it a good read, it barely digs deep into the problem or the solution, apart from scratching the surface, `the military surface'. Indeed the book did create waves in the political arena, which was probably due to its impeccable timing, but I cast serious doubts on the claims that it would be \"discussed for years to come\". *1   - http://www.newamericancentury.org/RebuildingAmericasDefenses.pdf *2   - http://www.climnet.org/news/march2001.html *3   - http://news.bbc.co.uk/1/hi/world/europe/1383385.stm *4   - http://212.58.226.30/1/hi/business/2052405.stm *5   - http://www.wsws.org/articles/1999/apr1999/yugo-a14.shtml *6   - http://www.atrocities.net/ *7   - home.att.net/~Resurgence/CIAtimeline.html *8   - www.serendipity.li/cia/cia_time.htm *9   - http://www.cia.gov/cia/publications/factbook/geos/ee.html *10 - http://www.wsws.org/articles/2000/dec2000/us-d30.shtml *11 - http://news.bbc.co.uk/1/hi/business/1263211.stm *12 - http://www.michaelmoore.com/words/index.php *13 - http://en.wikipedia.org/wiki/Henry_Kissinger *14 - http://www.thenation.com/blogs/capitalgames?bid=3&pid=17 |Positive :\n",
      "word : price |Negative : How the Other Half Lives was written over a century ago as an expose' of the appalling living conditions in the tenements of New York City. The author speaks with a crusader's zeal and with so much detail that the reader can visualize these tenements, these streets, and these people as if they were living today. This is a grim and moving portrayal of the lives of the men, the women and the children that inhabit these loathsome neighborhoods of New York City in the late 1800s.  He so graphically describes the filth, the sunless and airless rooms, the crowding, and the starvation that these places palpably exist for the reader and bring a chill to any heart.  Riis has a genuine concern for the tenement situation and understands these people's plight. His pictures are touching and meant to vividly show their misery. He, for the most part, blames the money-hungry landlords for these crowded conditions: \"How shall the love of God be understood by those who have been nurtured in sight only of the greed of man.\" (p. 266). He laments that the tenement is three quarters responsible for the misery of the poor. Then, after his extensive discourse, he offers three concrete cures for these dreadful conditions, something that many authors forget when they are enlightening readers. Riis states his purpose for writing this book himself on page 297 when he says, \"If this book shall have borne ever so feeble a hand in garnering a harvest of justice, it has served its purpose.\" The author apparently succeeded with his purpose, because in the flyleaf of this book the publisher tells us: \"This book helped bring about new revisions in the housing codes of the major U. S. cities.\" How does this book relate to adult education? By moving chronologically from the founding of America to the slums of NYC in the 1800s, are we to understand that education took a back seat to the accumulation of money during those hundred years? Certainly Riis posits that education is one of the solutions to this problem while at the same time implying that wealth accumulation and the lack of education has been the cause of this problem.  \"Thus the whole matter resolves itself once more into a question of education, all the more urgent because these people are poor, miserably poor almost to a man.\" (p. 147). He does not forget that this \"education\" is not only for the poor people, but also for the wealthy landlords. \"Clearly, it is a matter of education on the part of the landlord no less than the tenant.\" (p. 270). Riis' almost missionary language exhorts us to never allow greed to override a compassion for humanity.  \"It is a fight in which eternal vigilance is truly the price of liberty and the preservation of society.\" (p. 233). The relation of this book to the other units of study in this course could be explained as an example of how low mankind can descend if education is not considered one of the major cornerstones of a society. Although this book was exhaustive in its detail, it was easy to read while at the same time enlightening. I found myself enjoying the ranting tone and the fascinating lists of neighborhoods and the labels that Riis used for the different races. In a sense it reads like a Ginsberg poem; constantly hitting the reader with melodious lists of places and people from another era. Read these geographic names out loud and feel their rhythm: Jewtown, Bandit's Roost, Double Alley, The Bowery, The Bloody Sixth Ward, The Fourth, Fifth and Tenth Wards,  Blindman's Alley, The Bend, The Battery, Little Italy, The French Quarter, Hell's Kitchen, The West Side, Bottle Alley, Frog Hollow, Poverty Gap, Murderer's Alley, Gotham Court, The Old Brewery, Old Africa, Potter's Field, Blackwell's Island Asylum, Rogues' Gallery, Penitentiary Row, Chinatown. The list goes on. To someone reared in rural America, this chant sounds like a song about another country. Riis also has something to say about all the races of people that live in the tenements of New York City: Chinaman (he can't be taught), (Chinese (coolie--laundry business), Polish Jew (coops himself up in his den with his thermometer at boiling), Russian Jew, Jew (money is their God), Bohemian (poor, but thrifty), Blacks (like to gamble, but they are clean), Greek, Hebrews (tailors, all of them), Dutchman, Irish (like to drink and have expensive funerals), German (order loving), Swiss, Pasquales, Russian, Italian (a born gambler; lighthearted and gay), Swamp Angel (thieves), Street Arabs (army of homeless boys), Tramps and Toughs (the world owes them a living),and the Celtics. I was enthralled by the street lingo and the colloquialisms and Riis' comments about every race. I noticed one interesting similarity between the late 1800s and the present day. Riis says that the gap between the social classes is widening everyday. I hear people talking about the growing differences between the rich and the poor often these days. Does every generation think that is happening in their time? Not only should this book be used by people researching New York City`s tenements, but it should also be read by everyone seeking a blueprint for social change. It is a reminder to show us how hopeless the human circumstances can become when man's greed overrides his concern for human dignity. |Positive :\n"
     ]
    }
   ],
   "source": [
    "# TODO: you can fill your code for finding cases here\n",
    "positive_ngrams = []\n",
    "negative_ngrams = []\n",
    "\n",
    "for n, counts in top_100_reranked1.items():\n",
    "    positive_ngrams.append(counts[:1])\n",
    "    negative_ngrams.append(counts[-1:])\n",
    "\n",
    "for n, counts in top_100_reranked2.items():\n",
    "    positive_ngrams.append(counts[:1])\n",
    "    negative_ngrams.append(counts[-1:])\n",
    "\n",
    "indexes_pos = [0,4]\n",
    "indexes_neg = [0,4]\n",
    "# find sentences that have a positive n-gram and a negative prediction\n",
    "for pred_file in prediction_files:\n",
    "\n",
    "    with jsonlines.open(pred_file, mode=\"r\") as reader:\n",
    "        preds = [pr for pr in reader.iter()]\n",
    "        for pred in preds:\n",
    "\n",
    "            review_words = [word.strip(\"Ġ\") for word in tokenizer.tokenize(pred[\"review\"].lower()) if word.strip(\"Ġ\")]\n",
    "            original_review = pred[\"review\"]\n",
    "            \n",
    "\n",
    "            for idx in indexes_pos:\n",
    "               if positive_ngrams[idx][0][0] in review_words and pred[\"prediction\"] == \"positive\":\n",
    "                    print(\"word :\", positive_ngrams[idx][0][0], \"|Negative :\", original_review, \"|Positive :\")\n",
    "                    indexes_pos.remove(idx)\n",
    "\n",
    "            for idx in indexes_neg:\n",
    "               if negative_ngrams[idx][0][0] in review_words and pred[\"prediction\"] == \"negative\":\n",
    "                    print(\"word :\", negative_ngrams[idx][0][0], \"|Positive :\", original_review, \"|Negative :\")\n",
    "                    indexes_neg.remove(idx)\n",
    "\n",
    "            \n",
    "                \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c45cvlUqufRR"
   },
   "source": [
    "TODO🔻: (Write your case study discussions and answers to the questions here.)\n",
    "Three out of the 4 words that led to missclassification are generic words which can mean boath positive and negative meanings (would, end, price).. Hence we can conclude that the higher the n, the better we can understand the meaning of the n-gram and better we can classify it. \n",
    "\n",
    "-> The most robust is 4-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yND0DEfT-eXn"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 3: Annotate New Data (25 pts)**\n",
    "\n",
    "In this part, you will **annotate** the gold labels of some **new** SA data samples, and measure the degree of **agreement** between your and **one or two partners'** annotations.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQNXrRHr_3vV"
   },
   "source": [
    "## 🎯 Q3.1: **Write an Annotation Guideline (5 pts)**\n",
    "\n",
    "TODO🔻: Imagine that you are going to assign this annotation task to a crowdsourcing worker, who is completely not familiar with computer science and NLP. Think about how you are going to explain this annotation task to him in order to guide him do a decent job. Write an annotation guideline for such a worker who are going to do this task for you.\n",
    "\n",
    "**Note:** You should come up with your own guideline without the help of your partner(s) in later Part 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfkqNbUA_3vV"
   },
   "source": [
    "You will be given a list of texts (each text is called an \"instance\"). Your task is to read each instance carefully and decide if the sentiment expressed is **positive, negative** which translates to whether the writer is happy or frustrated about the item he is reviewing. To achieve this follow these guidelines:\n",
    "- Read carefully the text especially the **beginning and the ending** since they might contain the feeling of the reviewer or a summary of the reviewer point of view.\n",
    "- Look for positive adjectives and adverbs (e.g., \"great\", \"beautifully\") or negative ones (e.g., \"aweful\", \"terrible\").\n",
    "- Recommendation or complaints (e.g., \"I highly recommend this product\" or \"Don't buy this product\").\n",
    "- Use your judgement to decide which side is more present especially at the ending of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBWK4Bw__3vV"
   },
   "source": [
    "## 🎯 Q3.2: **Annotate Your Datapoints with Partner(s) (8 pts)**\n",
    "\n",
    "TODO🔻: Annotate 80 datapoints (20 in each domain of \"books\", \"dvd\", \"electronics\" and \"housewares\") assigned to you and your partner(s), by editing the value of the key **\"label\"** in each datapoint. You and your partner(s) should annotate **independently of each other**, i.e., each of you provide your own 80 annotations.\n",
    "\n",
    "Please find your assigned annotation dataset **ID** and **your partner(s)** according to this [list](https://docs.google.com/spreadsheets/d/1hOwBUb8XE8fitYa4hlAwq8mARZe3ZsL4/edit?usp=sharing&ouid=108194779329215429936&rtpof=true&sd=true). Your annotation dataset can be found [here](https://drive.google.com/drive/folders/1IHXU_v3PDGbZG6r9T5LdjKJkHQ351Mb4?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWhjTn2fQ5YE"
   },
   "source": [
    "**Name your annotated file as `<your_assigned_dataset_id>-<your_sciper_number>.jsonl`.**\n",
    "\n",
    "**You should also submit your partner's annotated file `<assigned_dataset_id>-<your_partner_sciper_number>.jsonl`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uyP0GtHe_3vW"
   },
   "source": [
    "## 🎯 Q3.3: **Agreement Measure (12 pts)**\n",
    "\n",
    "TODO🔻: Based on your and your partner's annotations in 3.2, calculate the [Cohen's Kappa](https://scikit-learn.org/stable/modules/model_evaluation.html#cohen-kappa) or [Krippendorff's Alpha](https://github.com/pln-fing-udelar/fast-krippendorff) (if you are in a group of three students) between the annotators on **each domain** and **across all domains**.\n",
    "\n",
    "**Note:** Cohen's Kappa or Krippendorff's Alpha interpretation\n",
    "\n",
    "0: No Agreement\n",
    "\n",
    "0 ~ 0.2: Slight Agreement\n",
    "\n",
    "0.2 ~ 0.4: Fair Agreement\n",
    "\n",
    "0.4 ~ 0.6: Moderate Agreement\n",
    "\n",
    "0.6 ~ 0.8: Substantial Agreement\n",
    "\n",
    "0.8 ~ 1.0: Near Perfect Agreement\n",
    "\n",
    "1.0: Perfect Agreement\n",
    "\n",
    "**Questions:**\n",
    "- What is the overall degree of agreement between you and your partner(s) according to the above interpretation of score ranges?\n",
    "- In which domain are disagreements most and least frequently happen between you and your partner(s)? Give some examples to explain why that is the case.\n",
    "- Are there possible ways to address the disagreements between annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5_2VlClO38Jt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7761194029850746\n"
     ]
    }
   ],
   "source": [
    "# Fill your code for calculating agreement scores here.\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "prediction_files = ['data/74-315196.jsonl', 'data/74-353333.jsonl']\n",
    "\n",
    "def calculate_agreement_scores(prediction_files):\n",
    "    agreements = []\n",
    "\n",
    "    with jsonlines.open(prediction_files[0], mode=\"r\") as reader:\n",
    "        preds1 = [pr for pr in reader.iter()]\n",
    "        predictions1 = [pr[\"label\"] for pr in preds1]\n",
    "\n",
    "    with jsonlines.open(prediction_files[1], mode=\"r\") as reader:\n",
    "        preds2 = [pr for pr in reader.iter()]\n",
    "        predictions2 = [pr[\"label\"] for pr in preds2]\n",
    "\n",
    "    # cohens kappa score\n",
    "    kappa = cohen_kappa_score(predictions1, predictions2)\n",
    "\n",
    "    return kappa\n",
    "\n",
    "\n",
    "agreements = calculate_agreement_scores(prediction_files)\n",
    "print(f'Score: {agreements}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreement Scores per domain: (0.7333333333333334, 0.7727272727272727, 0.6428571428571428, 0.7938144329896907)\n"
     ]
    }
   ],
   "source": [
    "def calculate_agreement_scores_per_domain(prediction_files):\n",
    "    agreements = []\n",
    "\n",
    "    with jsonlines.open(prediction_files[0], mode=\"r\") as reader:\n",
    "        preds1 = [pr for pr in reader.iter()]\n",
    "        predictions1 = [pr[\"label\"] for pr in preds1]\n",
    "\n",
    "    with jsonlines.open(prediction_files[1], mode=\"r\") as reader:\n",
    "        preds2 = [pr for pr in reader.iter()]\n",
    "        predictions2 = [pr[\"label\"] for pr in preds2]\n",
    "\n",
    "    # cohens kappa per domain\n",
    "    kappa1 = cohen_kappa_score(predictions1[:20], predictions2[:20])\n",
    "    kappa2 = cohen_kappa_score(predictions1[20:40], predictions2[20:40])\n",
    "    kappa3 = cohen_kappa_score(predictions1[40:60], predictions2[40:60])\n",
    "    kappa4 = cohen_kappa_score(predictions1[60:80], predictions2[60:80])\n",
    "\n",
    "    return kappa1, kappa2, kappa3, kappa4\n",
    "\n",
    "scores = calculate_agreement_scores_per_domain(prediction_files)\n",
    "print(f'Scores per domain: {scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The third domain 'electronics' is the domain that had the lowest agreement score while 'houseware' had the highest score. So electronics had the most disagreement and houseware had the least this might be due to the fact that houseware reviews are straight to the point unlike electronics where they tend to be longer and non bianry.\n",
    "\n",
    "- There are ways to address this issue like by implementing an adjudication process where a third annotator or program reviews the instances with disagreements and makes a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4wuRpHt-rQF"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "## **Part 4: Data Augmentation (20 pts)**\n",
    "\n",
    "Since we only used 20% of the whole dataset for training, which might limit the model performance. In the final part, we will try to enlarge the training set by **data augmentation**.  \n",
    "\n",
    "Specifically, we will **`Rephrase`** some current training samples using pretrained paraphraser. So that the paraphrased synthetic samples would preserve the semantic similarity while change the surface format.\n",
    "\n",
    "You can use the pretrained T5 paraphraser [here](https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQa2q1io_5Pk"
   },
   "source": [
    "## 🎯 Q4.1: **Data Augmentation with Paraphrasing (15 pts)**\n",
    "TODO🔻: Implement functions named `get_paraphrase_batch` and `get_paraphrase_dataset` with the details in the below two blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nTMdZ-azABk-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from tqdm import tqdm\n",
    "# get the given pretrained paraphrase model and the corresponding tokenizer (https://huggingface.co/humarin/chatgpt_paraphraser_on_T5_base)\n",
    "paraphrase_tokenizer = AutoTokenizer.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\")\n",
    "paraphrase_model = AutoModelForSeq2SeqLM.from_pretrained(\"humarin/chatgpt_paraphraser_on_T5_base\").to(device)\n",
    "\n",
    "def get_paraphrase_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    input_samples,\n",
    "    n,\n",
    "    repetition_penalty=10.0,\n",
    "    diversity_penalty=3.0,\n",
    "    no_repeat_ngram_size=2,\n",
    "    temperature=0.7,\n",
    "    max_length=256,\n",
    "    device='mps'):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphraser\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      input_samples: a batch (list) of real samples to be paraphrased\n",
    "      n: number of paraphrases to get for each input sample\n",
    "      for other parameters, please refer to:\n",
    "          https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Output: Tuple.\n",
    "      synthetic_samples: a list of paraphrased samples\n",
    "    '''\n",
    "\n",
    "    # TODO: implement para phrasing on a batch of imput samples\n",
    "    synthetic_samples = []\n",
    "    # Setting the model to evaluation mode and to the specified device\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # Processing each input sample\n",
    "    for sample in input_samples:\n",
    "        # Encode the input sample\n",
    "\n",
    "        if len(sample[\"review\"]) > max_length:\n",
    "            sample[\"review\"] = sample[\"review\"][:max_length]\n",
    "        input_ids = tokenizer.encode(\"paraphrase: \" + sample[\"review\"], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generating paraphrases\n",
    "        paraphrases = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=n,\n",
    "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            temperature=temperature,\n",
    "            diversity_penalty=diversity_penalty,\n",
    "            do_sample=False,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            early_stopping=True,\n",
    "            num_beams=2,\n",
    "            num_beam_groups=2\n",
    "        ) \n",
    "\n",
    "        # Decoding the generated ids to text and adding to the result list\n",
    "        for paraphrase in paraphrases:\n",
    "            synthetic_samples.append({\"review\": tokenizer.decode(paraphrase, skip_special_tokens=True), \"domain\": sample[\"domain\"], \"label\": sample[\"label\"]})    \n",
    "\n",
    "    return synthetic_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = 'data'\n",
    "data_train_path = os.path.join(data_dir, 'train_sa.jsonl')\n",
    "BATCH_SIZE = 8\n",
    "N_PARAPHRASE = 2\n",
    "\n",
    "def get_paraphrase_dataset(model, tokenizer, data_path, batch_size, n_paraphrase):\n",
    "    '''\n",
    "    Input\n",
    "      model: paraphrase model\n",
    "      tokenizer: paraphrase tokenizer\n",
    "      data_path: path to the `jsonl` file of training data\n",
    "      batch_size: number of input samples to be paraphrases in one batch\n",
    "      n_paraphrase: number of paraphrased sequences for each sample\n",
    "    Output:\n",
    "      paraphrase_dataset: a list of all paraphrase samples. Do not include the original training data.\n",
    "    '''\n",
    "    \n",
    "    paraphrase_dataset = []\n",
    "    batch = []\n",
    "    with jsonlines.open(data_path, mode='r') as reader:\n",
    "        for obj in tqdm(reader):\n",
    "            batch.append({\"review\": obj['review'], \"domain\": obj['domain'], \"label\": obj['label']})\n",
    "            if len(batch) == batch_size:\n",
    "                paraphrases = get_paraphrase_batch(\n",
    "                    model=model,\n",
    "                    tokenizer=tokenizer,\n",
    "                    input_samples=batch,\n",
    "                    n=n_paraphrase,\n",
    "                    device=device\n",
    "                )\n",
    "                paraphrase_dataset.extend(paraphrases)\n",
    "                batch = []\n",
    "\n",
    "\n",
    "    return paraphrase_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** run paraphrasing, which will take ~20-30 minutes using a T4 Colab GPU. But the running time could depend on various implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "1600it [46:27,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "paraphrase_dataset = get_paraphrase_dataset(paraphrase_model, paraphrase_tokenizer, data_train_path, BATCH_SIZE, N_PARAPHRASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "nGQsvD4dktVv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 3200 4800\n"
     ]
    }
   ],
   "source": [
    "# Original training dataset\n",
    "with jsonlines.open(data_train_path, \"r\") as reader:\n",
    "    origin_data = [dt for dt in reader.iter()]\n",
    "\n",
    "all_data = origin_data + paraphrase_dataset\n",
    "\n",
    "# Write all the original and paraphrased data samples into training dataset\n",
    "augmented_data_train_path = os.path.join(data_dir, 'augmented_train_sa.jsonl')\n",
    "with jsonlines.open(augmented_data_train_path, \"w\") as writer:\n",
    "    writer.write_all(all_data)\n",
    "\n",
    "assert len(all_data) == 3 * len(origin_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmFpfxrjWA1O"
   },
   "source": [
    "## 🎯 Q4.2: **Retrain RoBERTa Model with Data Augmentation (5 pts)** \n",
    "TODO🔻: Retrain the sentiment analysis model with the augmented (original+paraphrased), larger dataset :)\n",
    "\n",
    "**Note:** *Training on the augmented data will take about 15 minutes using a T4 Colab GPU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sz9gQSe8ANix"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4800it [00:01, 4371.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SA Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6400it [00:02, 2596.00it/s]\n",
      "/Users/aziz/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training:   0%|          | 0/600 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Training: 100%|██████████| 600/600 [03:24<00:00,  2.93it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training Loss: 0.554 | Validation Loss: 0.319\n",
      "Epoch 0 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2881.  319.]\n",
      " [ 344. 2856.]]\n",
      "F1: (89.68%, 89.60%) | Macro-F1: 89.64%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [03:12<00:00,  3.11it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:54<00:00, 14.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Training Loss: 0.369 | Validation Loss: 0.286\n",
      "Epoch 1 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2818.  382.]\n",
      " [ 248. 2952.]]\n",
      "F1: (89.95%, 90.36%) | Macro-F1: 90.15%\n",
      "Model Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [03:10<00:00,  3.15it/s]\n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Training Loss: 0.192 | Validation Loss: 0.631\n",
      "Epoch 2 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[3012.  188.]\n",
      " [ 530. 2670.]]\n",
      "F1: (89.35%, 88.15%) | Macro-F1: 88.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 600/600 [07:36<00:00,  1.31it/s]  \n",
      "Evaluation: 100%|██████████| 800/800 [00:55<00:00, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Training Loss: 0.085 | Validation Loss: 0.652\n",
      "Epoch 3 SA Validation:\n",
      "Confusion Matrix:\n",
      "[[2782.  418.]\n",
      " [ 236. 2964.]]\n",
      "F1: (89.48%, 90.06%) | Macro-F1: 89.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Re-train a RoBERTa SA model on the augmented training dataset\n",
    "learning_rate = 1e-5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"FacebookAI/roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "train(train_dataset= SADataset(\"data/augmented_train_sa.jsonl\",tokenizer= tokenizer), dev_dataset=SADataset(\"data/test_sa.jsonl\", tokenizer= tokenizer), model=model, device=device,\n",
    "      batch_size=8, epochs=4, learning_rate=1e-5, warmup_percent=0.3, max_grad_norm=1.0,\n",
    "      model_save_root='models/augmented/', tensorboard_path=\"./tensorboard/part4_lr{}\".format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PRvIB7ZAyaE"
   },
   "source": [
    "TODO🔻: Discuss your results by answering the following questions\n",
    "\n",
    "- Compare the performances of models in Part 1 and Part 4. Does the data augmentation help with the performance and why (give possible reasons)?\n",
    "- No matter whether the data augmentation helps or not, list **three** possible ways to improve our current data augmentation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5-dVSsniH9N"
   },
   "source": [
    "-The performance of the model in part 4 has slightly improved compared to the one in part 1. It achieved approx 0.06% increase in F1 score which was insignifanct. So we can conclude that the data augmentation didn't significantly help the model in our case.\n",
    "\n",
    "- Ways to improve data augmentation model:\n",
    "\n",
    "• Replace words in the text with their synonyms to create a new sentence with similar meaning. \n",
    "\n",
    "• Translate the text into another language and then back to english. This introduces linguistic variations and can significantly change the sentence structure.\n",
    "\n",
    "• Create or generate using the previous model new sentences that contrast with the original text in terms of sentiment \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CIeN_kaOQl"
   },
   "source": [
    "<div style=\"padding:15px 20px 20px 20px;border-left:3px solid orange;background-color:#fff5d6;border-radius: 20px;color:#424242;\">\n",
    "\n",
    "### **5 Upload Your Notebook, Data and Models**\n",
    "\n",
    "Please upload your filled jupyter notebook in your GitHub Classroom repository, **with all cells run and output results shown**.\n",
    "\n",
    "**Note:** We are **not** responsible for re-running the cells in your notebook.\n",
    "\n",
    "Please also submit all your **datasets** **(anotated and augmented)**, as well as **all your trained models** in Part 1 and Part 4, in your GitHub Classroom repository.\n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
